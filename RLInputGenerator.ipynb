{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768250e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "208cb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BSTEnvironment:\n",
    "    def __init__(self, max_tree_size=16):\n",
    "        self.max_tree_size = max_tree_size\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = []\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def step(self, states, action):\n",
    "        new_state = self.apply_action(states, action)\n",
    "        reward = self.calculate_reward(self.apply_action(state, action))\n",
    "        done = len(new_state) == self.max_tree_size  # Episode ends when the sequence is complete\n",
    "        return new_state, reward, done\n",
    "\n",
    "    def apply_action(self, states, action):\n",
    "        new_state = np.append(states, action)\n",
    "        return new_state\n",
    "\n",
    "    def calculate_reward(self, new_state):\n",
    "        reward = 1.0 if self._check_property(new_state) else 0.0\n",
    "        return reward\n",
    "    def _check_property(self,new_state):\n",
    "        # Check if the sequence forms a valid BST\n",
    "        return self._is_bst(new_state)\n",
    "\n",
    "    def _is_bst(self, sequence):\n",
    "        if len(sequence) < 3:\n",
    "            return True \n",
    "        last_value = sequence[-1]\n",
    "        left_subtree = [val for val in sequence if val < last_value]\n",
    "        right_subtree = [val for val in sequence if val > last_value]\n",
    "        return self._is_bst(left_subtree) and self._is_bst(right_subtree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2a56894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "class RLAgent:\n",
    "    def __init__(self, input_size, output_size, max_tree_size=16):\n",
    "        self.max_tree_size = max_tree_size\n",
    "        self.model = self._build_model(input_size, output_size)\n",
    "        self.model_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    def _build_model(self, input_size, output_size):\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu', input_shape=(input_size,)),\n",
    "            layers.Dense(output_size, activation='softmax')\n",
    "        ])\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy')\n",
    "        return model\n",
    "\n",
    "    def select_action(self, state):\n",
    "        action = np.array(np.random.randint(1, 101))\n",
    "        return action\n",
    "    def predict_action(self, state):\n",
    "        print(state)\n",
    "        ac = []\n",
    "        for _ in range(self.max_tree_size):\n",
    "            # Use the model to predict the next action\n",
    "            action_prob = self.model.predict(np.array(state).reshape(1, -1))\n",
    "            action = np.random.choice(len(action_prob), p=action_prob)\n",
    "            ac.append(action)\n",
    "        return ac\n",
    "\n",
    "    def train(self, states, actions, rewards):\n",
    "        discounted_rewards = self._calculate_discounted_rewards(rewards)\n",
    "        actions_one_hot = tf.one_hot(actions, depth=self.model.output_shape[1])\n",
    "        advantages = discounted_rewards\n",
    "\n",
    "    # Ensure the states are properly shaped\n",
    "        states = np.reshape(states, (1, -1))\n",
    "        with tf.GradientTape() as tape:\n",
    "            policy_prob = self.model(states)\n",
    "            action_prob = tf.reduce_sum(actions_one_hot * policy_prob, axis=1)\n",
    "            loss = -tf.reduce_sum(tf.math.log(action_prob) * advantages)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.model_optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "\n",
    "    def _calculate_discounted_rewards(self, rewards, gamma=0.99):\n",
    "        discounted_rewards = np.zeros_like(rewards, dtype=np.float32)\n",
    "        running_add = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            running_add = running_add * gamma + rewards[t]\n",
    "            discounted_rewards[t] = running_add\n",
    "        return discounted_rewards\n",
    "    def generate_rl_guided_input(self, num_samples=5):\n",
    "        guided_inputs = []\n",
    "        for _ in range(num_samples):\n",
    "            state = self.generate_state()\n",
    "            action = self.predict_action(state)\n",
    "            guided_inputs.append(self.apply_action([], action))\n",
    "\n",
    "        return guided_inputs\n",
    "    \n",
    "    def generate_state(self):\n",
    "        return np.array(random.sample(range(1, 101), self.max_tree_size))\n",
    "\n",
    "    def apply_action(self, state, action):\n",
    "        # Apply the action by inserting the chosen action into the state\n",
    "        new_state = state + [action]\n",
    "        return new_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "035f619e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate RLAgent with desired input and output sizes\n",
    "env = BSTEnvironment()\n",
    "gen_test_inp=[]\n",
    "for i in range(5):\n",
    "    inp_size=random.randint(10, 90)\n",
    "    agent = RLAgent(input_size=inp_size, output_size=101, max_tree_size=inp_size)\n",
    "    for episode in range(2):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        states,actions,rewards=np.array([]),np.array([]),np.array([])\n",
    "        while True:\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(states, action)\n",
    "            rewards=np.append(rewards,reward)\n",
    "            actions=np.append(actions,action)\n",
    "            states= next_state\n",
    "        # Update the agent with the experience\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        gen_test_inp.append(states)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "083f59e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed Binary Search Tree:\n",
      "Root: 26.0\n",
      "    L--- 2.0\n",
      "        R--- 13.0\n",
      "            L--- 8.0\n",
      "                R--- 12.0\n",
      "            R--- 18.0\n",
      "                R--- 19.0\n",
      "    R--- 94.0\n",
      "        L--- 55.0\n",
      "            L--- 27.0\n",
      "                R--- 33.0\n",
      "                    R--- 51.0\n",
      "            R--- 56.0\n",
      "                R--- 77.0\n",
      "        R--- 96.0\n",
      "Constructed Binary Search Tree:\n",
      "Root: 13.0\n",
      "    L--- 3.0\n",
      "    R--- 59.0\n",
      "        L--- 27.0\n",
      "            L--- 25.0\n",
      "            R--- 31.0\n",
      "                R--- 40.0\n",
      "                    R--- 43.0\n",
      "                        R--- 58.0\n",
      "        R--- 92.0\n",
      "            L--- 63.0\n",
      "                R--- 82.0\n",
      "                    L--- 81.0\n",
      "                        L--- 74.0\n",
      "                    R--- 86.0\n",
      "Constructed Binary Search Tree:\n",
      "Root: 75.0\n",
      "    L--- 30.0\n",
      "        L--- 10.0\n",
      "            L--- 2.0\n",
      "                R--- 4.0\n",
      "                    R--- 6.0\n",
      "            R--- 25.0\n",
      "                L--- 17.0\n",
      "                    L--- 11.0\n",
      "                R--- 29.0\n",
      "        R--- 44.0\n",
      "            R--- 62.0\n",
      "    R--- 94.0\n",
      "        L--- 79.0\n",
      "Constructed Binary Search Tree:\n",
      "Root: 50.0\n",
      "    L--- 29.0\n",
      "        L--- 16.0\n",
      "            R--- 22.0\n",
      "                L--- 18.0\n",
      "        R--- 41.0\n",
      "            L--- 38.0\n",
      "            R--- 46.0\n",
      "    R--- 68.0\n",
      "        L--- 64.0\n",
      "            L--- 59.0\n",
      "        R--- 72.0\n",
      "            R--- 83.0\n",
      "                R--- 96.0\n",
      "                    L--- 93.0\n",
      "Constructed Binary Search Tree:\n",
      "Root: 9.0\n",
      "    R--- 98.0\n",
      "        L--- 22.0\n",
      "            R--- 96.0\n",
      "                L--- 91.0\n",
      "                    L--- 87.0\n",
      "                        L--- 24.0\n",
      "                            R--- 83.0\n",
      "                                L--- 71.0\n",
      "                                    L--- 25.0\n",
      "                                        R--- 63.0\n",
      "                                            L--- 56.0\n",
      "                                                L--- 54.0\n",
      "                                    R--- 73.0\n",
      "        R--- 100.0\n"
     ]
    }
   ],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "def construct_bst(root, values):\n",
    "    for value in values:\n",
    "        root = insert_into_bst(root, value)\n",
    "    return root\n",
    "\n",
    "def insert_into_bst(root, value):\n",
    "    if root is None:\n",
    "        return TreeNode(value)\n",
    "\n",
    "    if value < root.value:\n",
    "        root.left = insert_into_bst(root.left, value)\n",
    "    elif value > root.value:\n",
    "        root.right = insert_into_bst(root.right, value)\n",
    "\n",
    "    return root\n",
    "\n",
    "def display_bst(root, level=0, prefix=\"Root: \"):\n",
    "    if root is not None:\n",
    "        print(\" \" * (level * 4) + prefix + str(root.value))\n",
    "        if root.left is not None or root.right is not None:\n",
    "            display_bst(root.left, level + 1, \"L--- \")\n",
    "            display_bst(root.right, level + 1, \"R--- \")\n",
    "\n",
    "for i in range(5):\n",
    "    bst_root = None\n",
    "    input_data=gen_test_inp[i]\n",
    "    for value in input_data: \n",
    "        bst_root = insert_into_bst(bst_root, value)\n",
    "    print(\"Constructed Binary Search Tree:\")\n",
    "    display_bst(bst_root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bf40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08944659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
